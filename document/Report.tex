\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,algorithm,hyperref,xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}
\usepackage{graphicx}

\title{Distributed Ensembles for Image Classification}
\usepackage{fullpage,comment}
\author{Andrea Burns \and Gavin Brown \and Iden Kalemaj}
\date{\today}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\eps}{\varepsilon}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\Sec}[1]{\hyperref[sec:#1]{Section\,\ref*{sec:#1}}} %section
\newcommand{\Obs}[1]{\hyperref[obs:#1]{Observation~\ref*{obs:#1}}} %section
\newcommand{\Eqn}[1]{\hyperref[eq:#1]{Eq. (\ref*{eq:#1})}} %equation
\newcommand{\Fig}[1]{\hyperref[fig:#1]{Fig.\,\ref*{fig:#1}}} %figure
\newcommand{\Tab}[1]{\hyperref[tab:#1]{Table\,\ref*{tab:#1}}} %table
\newcommand{\Thm}[1]{\hyperref[thm:#1]{Theorem\,\ref*{thm:#1}}} %theorem
\newcommand{\Fact}[1]{\hyperref[fact:#1]{Fact\,\ref*{fact:#1}}} %fact
\newcommand{\Lem}[1]{\hyperref[lem:#1]{Lemma\,\ref*{lem:#1}}} %lemma
\newcommand{\Prop}[1]{\hyperref[prop:#1]{Prop.~\ref*{prop:#1}}} %property
\newcommand{\Cor}[1]{\hyperref[cor:#1]{Corollary~\ref*{cor:#1}}} %corollary
\newcommand{\Conj}[1]{\hyperref[conj:#1]{Conjecture~\ref*{conj:#1}}} %conjecture
\newcommand{\Def}[1]{\hyperref[def:#1]{Definition~\ref*{def:#1}}} %
\newcommand{\Alg}[1]{\hyperref[alg:#1]{Algorithm~\ref*{alg:#1}}} %algorithm
\newcommand{\Pro}[1]{\hyperref[pro:#1]{Procedure~\ref*{pro:#1}}} %procedure
\newcommand{\Ex}[1]{\hyperref[ex:#1]{Ex.~\ref*{ex:#1}}} %example
\newcommand{\Clm}[1]{\hyperref[clm:#1]{Claim~\ref*{clm:#1}}} %example
\newcommand{\Stp}[1]{\hyperref[step:#1]{Step~\ref*{step:#1}}}
\newcommand{\Ch}[1]{\hyperref[chap:#1]{Chapter~\ref*{chap:#1}}}


\algnewcommand\algorithmicinput{\textbf{INPUT:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{\textbf{OUTPUT:}}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\begin{document}
\maketitle
\section{Links}
\begin{itemize}
	\item Github link: https://github.com/gavinrbrown1/distributed-ensemble-project
	\item Slice name: Project5
\end{itemize}

\section{Introduction}

The general goal of image classification is to determine which of several fixed classes an image belongs to. 
One common benchmark is the CIFAR-10 dataset \cite{cifar10}, which contains 32$\times$32 pixel images of ten different classes: airplanes, automobiles, birds, cats, deer, dogs, frogs, horse, ship, truck.
We've included a selection of these images in Figure~\ref{fig:cifar10_images}.
Image classification is an important machine learning task, as it is an initial step toward visual scene understanding -- a skill needed to interact with and reason about an environment (use cases include robot assistants, autonomous vehicles, etc).

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{image0.jpeg}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{image3.jpeg}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{image5.jpeg}
    \end{subfigure}
    \caption{A few images from CIFAR-10. From left to right: a dog, a truck, and an airplane.}
    \label{fig:cifar10_images}
\end{figure}

Ensembling is a popular way to improve the performance of a machine learning task; an ensemble is a set of different classifiers that are ‘combined’ to make a collective, better decision on the task. 
Such models are also sometimes called ``committees;'' for a reference see Chapter 14 of \cite{bishop}.
%The core idea is that combining many weak learners results in a strong learner. 
In this experiment we distribute 4 independent classifiers across different network nodes (servers) and take a majority vote of their decisions as the final classification result.
This distributed system allows us to investigate latency versus accuracy: by only taking the votes from classifiers that have responded within a certain timeout period, we force decisions to be faster but possibly lose accuracy.
Lastly, we try hand-crafted caching tools that can act as short-cut decisions, to see the impact on end-to-end delay.

\begin{figure}
    \centering
    \includegraphics[scale=0.2]{networksArchitecture.png}
    \caption{Here we have a high level representation of the GENI architecture we set up. The primary components include a client, a manager, a caching mechanism which resides on the manager, and four classifiers which the manager can communicate with. Refer to text to see how these nodes interact.}
    \label{fig:architecture}
\end{figure}

A client wishing to receive classification for an image connects to a manager and sends the image to the manager. The manager then checks if the cache can be utilized before sending copies of the image to the classifiers. If the image already exists in the cache, or a close enough image exists (zero-normalized cross correlation coefficient is deemed high enough between two images), then it can simply return the cached decision. Otherwise, the manager will start communication with all four classifiers and will aggregate their results into a majority vote. This decision will then be added to the cache and communicated back to the waiting client. The architecture flow can be view in Figure~\ref{fig:architecture}, and below we have a Finite State Machine characterizing the behavior of our system. The classifiers are grouped under the same state since they behave identically up to randomness.

{\bf LEARNING OUTCOME GO HERE.} 
They asked for them in the project document, so they'll surely be worth like 5\% of the overall grade.

\section{Experimental methodology}

\subsection{Caching and Sampling}

When choosing how to test the performance of our system, we must specify how we select images to be classified.
One method is to sample them uniformly at random from our test set, which matches how machine learning models are usually evaluated.
This may not reflect how the system is used in practice: some images may occur far more than others, such as a automated security system that spends much of its time examining the same empty hallways.
To model this we also sample from a prototypically ``long-tailed'' distribution, a simple form of the Zipf distribution discussed in Chapter 16 of \cite{mitzenmacher}, where each image $i$ is assigned a weight $w_i\in 1\ldots, N$
    and is sampled with probability $p(i)\propto \frac{1}{w_i}$.
Thus some images will keep reappearing while others will be very rare.

The two key components of the cache are how we compare a new image from the client to the existing cache images, and how we choose to update the cache over time. To compare a new image to the cache images, the Zero-Normalized Cross-Correlation coefficient (ZNCC) is used. This is a image similarity metric that is invariant to brightness differences; its values range $[0,1]$, where a ZNCC value of one means the two images being compared are the same. We deem two images similar enough to use the cached decision if their ZNCC is greater than $0.9$. In the case that this threshold is not reached, we do not use the cache and have to communicate with the classifiers. Any time there is a cache miss, the new image is going to be added to the cache once the ensembled decision arrives. This means that we keep adding new samples to the cache until it is full. We experiment with cache sizes of 10, 50, and 100 to see the effects on latency. Once the cache is full, we need a protocol to kick out the least important samples being stored. We want our cache to keep track of the most useful image, decision pairs. 

We decide to kick out the \textbf{pair} of closest images in terms of L2/Euclidean distance. We can think of this pair as being the elast informative, since they are most similar to each other, and ideally our cache would include as much visual variety as possible. It is possible to use other image distance metrics, but for simplicity we stick with Euclidean. We remove two images, pulling inspiration from the many cases of exponential backoff we have seen in case. We note that this backoff isn't exponential. To optimize the computation of pairwise distances between images in the cache, we only compute the full $(n) \times (n-1)$ pairwise (where $n$ is the number of images in the cache) upon initialization. The first full cache results in the calculation of these distances, which are then stored in a text file. These values are only updated as samples are added or removed to the cache, never redundantly re-computing pairwise distances.

\section{Results}

\subsection{Usage instructions}



\subsection{Analysis results}

\section{Conclusions and Future Extensions}

We found that developing this system and running the experiments was a very rewarding process.
It was enjoyable to see the natural way our research interests of vision, machine learning, and probabilistic algorithms mapped neatly onto the concepts we've learned in class.
We also learned a lot through the process of pulling together the many components and defining our own protocols to interact across machines.
The results mostly matched what we expected, showing clear tradeoffs between accuracy, space requirements, and latency.

{\bf Was anything in the plots surprising?
Probably something should have surprised us.}

Here are some extensions:
\begin{itemize}
    \item Add more classifiers, allow them to scale up and down.
    \item Implement faster caching algorithms, and/or more sophisticated measures of image closeness.
\end{itemize}

\section{Division of labor}

Andrea, Iden, and Gavin decided on the project topic and wrote the proposal and report together.
All high-level decisions were made together: network topology, communication protocols, experiments, etc.
Much of the coding was done in pairs, as well.
The focus of specific sections were as follows: Andrea created the code to control caching.
Iden wrote the bulk of the socket code that allowed the machines to communicate.
Gavin wrote the code to handle the prediction as well as modeling error and delay on the part of the classifier.


\begin{thebibliography}{1}
    \bibitem{bishop}
    Bishop, Christopher M. Pattern recognition and machine learning. Springer, 2006.

    \bibitem{cifar10}
    Krizhevsky, Alex, and Geoffrey Hinton. Learning multiple layers of features from tiny images. Vol. 1. No. 4. Technical report, University of Toronto, 2009.

    \bibitem{mitzenmacher}
    Mitzenmacher, Michael, and Eli Upfal. Probability and computing: randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017.

\end{thebibliography}



\end{document}
